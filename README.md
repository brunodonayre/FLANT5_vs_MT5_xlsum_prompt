# FLANT5_vs_MT5_xlsum_prompt
# 游닗 Comparaci칩n de Modelos de Resumen de Noticias en Espa침ol

Este proyecto eval칰a y compara el desempe침o de modelos de lenguaje basados en T5 y mT5 para la tarea de resumen autom치tico de noticias pol칤ticas en espa침ol, utilizando m칠tricas como ROUGE y BERTScore.

## 游늷 Modelos Evaluados

1. **mT5-base** (`google/mt5-base`)  
   Modelo multiling칲e general sin ajuste para resumen.

2. **mT5-XLSum** (`csebuetnlp/mT5_multilingual_XLSum`)  
   Modelo entrenado espec칤ficamente para resumen multiling칲e de noticias.

3. **mT5-XLSum + Prompt Mejorado**  
   Misma arquitectura que el anterior, pero con ajustes en el prompt para mejorar la calidad del resumen generado.

## 游빍 Dataset

Se utiliz칩 un conjunto de noticias pol칤ticas reales en espa침ol, con res칰menes de referencia (`resumen_gold`) elaborados manualmente para benchmarking.

## 游늳 M칠tricas de Evaluaci칩n

- **ROUGE-1 / ROUGE-2 / ROUGE-L / ROUGE-Lsum**: mide el solapamiento literal de n-gramas entre el resumen generado y el de referencia.
- **BERTScore (Precision / Recall / F1)**: mide similitud sem치ntica usando embeddings de BERT.

## 游 Resultados

| Modelo              | ROUGE-1 | ROUGE-2 | ROUGE-L | BERTScore-F1 |
|---------------------|---------|---------|---------|--------------|
| mT5-base            | 0.0553  | 0.0092  | 0.0474  | 0.5655       |
| mT5-XLSum           | 0.2432  | 0.0808  | 0.1607  | 0.6589       |
| mT5-XLSum + Prompt  | 0.2276  | 0.0753  | 0.1581  | 0.6590       |

### 游댌 Interpretaci칩n

- **ROUGE** destaca el modelo `mT5-XLSum` como el m치s fiel al resumen original.
- **BERTScore** muestra que el modelo con prompt mejorado genera res칰menes m치s naturales y legibles, con una leve mejora en F1 sem치ntico.

游늷 En resumen:  
ROUGE mide fidelidad textual, BERTScore mide calidad sem치ntica.  
Si buscas res칰menes m치s legibles, el prompt mejorado es una excelente opci칩n.

## 游 C칩mo usar

1. Instala las dependencias:

```bash
pip install -r requirements.txt
